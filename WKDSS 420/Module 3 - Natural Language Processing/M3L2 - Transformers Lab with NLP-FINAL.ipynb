{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f9c9509",
   "metadata": {},
   "source": [
    "# M3L2 Transformers Lab\n",
    "In this lab, we will practice how to download various models from the open source HuggingFace repository (https://huggingface.co/).  Please check out the website and click on the **Models** and **Datasets** tab to familiarize yourself with the models we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4f052a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e68fcf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64798fc",
   "metadata": {},
   "source": [
    "### Section 1.1 - Sentiment Analysis \n",
    "\n",
    "First, let's look at using a transformer for Sentiment Analysis.  This task will take in a sentence and classify it as positive or negative.  Some models will output other classes, such as \"neutral\" or other labels depending on how they were trained.  You can go to the huggingface website for each model and see what the expected output classes will be, along with tips on how to use these models.  \n",
    "\n",
    "The default classifier is \"distilbert-base-uncased-finetuned-sst-2-english\", which returns a 2 class output (positive or negative sentiment) of the sentence that you supply.  \n",
    "\n",
    "We will start with the simplest way to use a model, with a feature called a ***pipeline***.  These are pre-trained models, so there is no training necessary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eccf012e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319d3eef2fcf4e7dbb109625764138eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiren\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hiren\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d17dc39cb2f14f0599d5faddc1197b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39c2079a9a64ecd85262b09d2113223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea13651d7560489d97d86546830ecd31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_seed(10)\n",
    "classifier = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7116fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9995855689048767}]\n"
     ]
    }
   ],
   "source": [
    "res = classifier(\"I am mad.\")\n",
    "\n",
    "print(res) # tells you sentiment of the sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8592d700",
   "metadata": {},
   "source": [
    "### Section 1.2 - Load different Sentiment Analysis Model\n",
    "We will see how to change the model.  This model was trained on financial data, and also on 3 classes - positive, negative and neutral.  These differences from the previous model will become apparent in the results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaa1b6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at ProsusAI/finbert and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'positive', 'score': 0.37243393063545227}]\n"
     ]
    }
   ],
   "source": [
    "set_seed(10)\n",
    "classifier2 = pipeline(task='sentiment-analysis', model='ProsusAI/finbert') \n",
    "res = classifier2(\"I am mad.\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8c57ed",
   "metadata": {},
   "source": [
    "So the classifier doesn't get this right.  It thinks \"I am mad\" is a *positive* result.  There are 3 classes, so random guessing is 33%.  So here, it predicts positive by 37% or just better than random guessing.  \n",
    "\n",
    "However, if we were to use a prompt that is more financial, you might get better results: https://huggingface.co/ProsusAI/finbert?text=I+am+mad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f7db43",
   "metadata": {},
   "source": [
    "### Section 2 - Text Generation\n",
    "In this section, let's explore how to use transformers for text generation, given a specific prompt.\n",
    "\n",
    "This is the default classifier for text generation, where you supply a seed and see what you get.  GPT2 is the default model that is loaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d4694c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I like data science because \\xa0it's easy and\\xa0 it's really clear \\xa0that this is a good thing.\\xa0 One example of \\xa0data science being able to build large\\xa0data,\\xa0 was a report by a\\xa0\"},\n",
       " {'generated_text': 'Hello, I like data science because \\xa0everything \\xa0occurs in the context of a complex \\xa0data \\xa0data-coding system.\"\\nBut some things don\\'t make sense. You have to think about some of the things that'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(10)\n",
    "generator(\"Hello, I like data science because \", max_length=50, num_return_sequences=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a24bbe5",
   "metadata": {},
   "source": [
    "Let's try another classifier.  ***Distilgpt2*** is a much smaller classifier.  Let's see how it does with the same prompt and seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e6a8f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"I love data science because, in my book, they don't have to do it myself. But, to have the data available by email and, I mean, how would people want to know about, say, a new drug, or do the\"},\n",
       " {'generated_text': \"I love data science because, as scientists, I believe it is important that any new knowledge in the history of the human world, as I write, has not only brought to us the many facts of life. In fact, it's about themâ€”\"}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(10)\n",
    "\n",
    "generator = pipeline('text-generation', model='distilgpt2')\n",
    "generator(\"I love data science because,\", max_length=50, num_return_sequences=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae95af8",
   "metadata": {},
   "source": [
    "As you can see, the performance can be very different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d020e5",
   "metadata": {},
   "source": [
    "### Section 3 - Fine tuning the model\n",
    "In this section, we will show how to fine tune a model to fit the data that is relevant to your application.\n",
    "\n",
    "We will be using a reduced BERT transformer called distilbert-base-uncased-finetuned-sst-2-english.  Documentation can be found here: https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english.  This model was chosen becuase it is small in size while still being comparable in performance to the full BERT model.  The small size will make it easier to train on a single laptop in a reasonable amount of time.\n",
    "\n",
    "This model was trained on the *glue* and *sst2* datasets, which are made up of generalized language sentences and phrases.  \n",
    "\n",
    "Here are the steps we will be taking:\n",
    "- Load sentiment-analysis transformer and conduct baseline test\n",
    "- Train transformer on new dataset, IMDB, which is made up of movie reviews\n",
    "- Test transformer on same text as in baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d647a6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load model and baseline performance\n",
    "set_seed(10)\n",
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english' \n",
    "classifier = pipeline(\"text-classification\", model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a3481af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.5186458826065063}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMDB database quote: \"I can't believe that those praising this movie herein aren't thinking of some other film.\"\n",
    "# This is reworded below so that we are not training and testing on the same words.  \n",
    "classifier(\"Your praise would be better for another film.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fd79b4",
   "metadata": {},
   "source": [
    "Next let's retrain the classifier on the IMDB movie review dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2bda924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a size from 0-25K.  Here, I'm choosing a small number for demonstration purposes\n",
    "test_size=50\n",
    "train_size=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b675329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was great to see some of my favorite stars of 30 years ago including John Ritter, Ben Gazarra and Audrey Hepburn. They looked quite wonderful. But that was it. They were not given any characters or good lines to work with. I neither understood or cared what the characters were doing.<br /><br />Some of the smaller female roles were fine, Patty Henson and Colleen Camp were quite competent and confident in their small sidekick parts. They showed some talent and it is sad they didn't go on to star in more and better films. Sadly, I didn't think Dorothy Stratten got a chance to act in this her only important film role.<br /><br />The film appears to have some fans, and I was very open-minded when I started watching it. I am a big Peter Bogdanovich fan and I enjoyed his last movie, \"Cat's Meow\" and all his early ones from \"Targets\" to \"Nickleodeon\". So, it really surprised me that I was barely able to keep awake watching this one.<br /><br />It is ironic that this movie is about a detective agency where the detectives and clients get romantically involved with each other. Five years later, Bogdanovich's ex-girlfriend, Cybil Shepherd had a hit television series called \"Moonlighting\" stealing the story idea from Bogdanovich. Of course, there was a great difference in that the series relied on tons of witty dialogue, while this tries to make do with slapstick and a few screwball lines.<br /><br />Bottom line: It ain't no \"Paper Moon\" and only a very pale version of \"What's Up, Doc\".\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"imdb\", )\n",
    "#dataset = load_dataset(\"imdb\")\n",
    "dataset_train = dataset[\"train\"][0:train_size]  # Just take the training split for now\n",
    "print(dataset_train['text'][10])\n",
    "print(dataset_train['label'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a235ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Next we need to tokenize the new IMDB dataset in the format of the transformer\n",
    "'''\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Using DistilBERT as it is 2.5x faster to train than the base BERT model.  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenized_data = tokenizer.batch_encode_plus(dataset_train[\"text\"], return_tensors=\"np\", \n",
    "                                             padding=True, max_length=512, truncation=True )\n",
    "# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras\n",
    "tokenized_data = dict(tokenized_data)\n",
    "\n",
    "labels_train = np.array(dataset_train[\"label\"])  # Label is already an array of 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fbe7dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 170s 36s/step - loss: 0.1181\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c78a238760>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Train the model with the new tokenized text'''\n",
    "\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load and compile our model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "# Lower learning rates are often better for fine-tuning transformers\n",
    "model.compile(optimizer=Adam(3e-5)) \n",
    "model.fit(tokenized_data, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa77a23",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43b91fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = load_dataset(\"imdb\")\n",
    "dataset_test = dataset[\"test\"][0:test_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b15cc2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''tokenize the test data'''\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Using DistilBERT as it is 2.5x faster to train than the base BERT model.  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenized_data = tokenizer.batch_encode_plus(dataset_test[\"text\"], return_tensors=\"np\", \n",
    "                                             padding=True, max_length=512, truncation=True )\n",
    "# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras\n",
    "tokenized_test_data = dict(tokenized_data)\n",
    "\n",
    "labels_test = np.array(dataset_test[\"label\"])  # Label is already an array of 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6f13e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 512)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test_data['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22071a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 29s 10s/step\n"
     ]
    }
   ],
   "source": [
    "# Now you can do predictions like in Keras\n",
    "ypred = model.predict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cc9412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs are in logits, so you need to use a softmax to get predictions\n",
    "import tensorflow as tf\n",
    "ypred_predictions = tf.nn.softmax(ypred.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3217b000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 2), dtype=float32, numpy=\n",
       "array([[9.9980921e-01, 1.9079298e-04],\n",
       "       [9.9336433e-01, 6.6356286e-03],\n",
       "       [9.9980825e-01, 1.9171991e-04],\n",
       "       [9.9949467e-01, 5.0532701e-04],\n",
       "       [2.3088291e-01, 7.6911712e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred_predictions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7aec8ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now use argmax to get the label depending on which class gets the maximum prediction\n",
    "y_test_pred_labels = np.argmax(ypred_predictions, axis=1)\n",
    "y_test_pred_labels[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f36d8441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare to the true data\n",
    "labels_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c9a487f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 29s 10s/step - loss: 0.0304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.030380595475435257"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the overall accuracy\n",
    "model.evaluate(tokenized_test_data, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39c4b0a",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "Using only 50 training and test observations, performance is low.  Also we only had 1 epoch.  If you have a GPU or a more powerful computing platform, you may want to use more observations and run multiple epochs to see if that improves performance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41789be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''tokenize the test data'''\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Using DistilBERT as it is 2.5x faster to train than the base BERT model.  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenized_data = tokenizer.encode_plus(\"Your praise would be better for another film.\", return_tensors=\"np\", \n",
    "                                             padding='max_length', max_length=512,truncation=True) #\n",
    "# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras\n",
    "tokenized_test_data = dict(tokenized_data)\n",
    "\n",
    "labels_test = 0# np.array(dataset_test[\"label\"])  # Label is already an array of 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1566d489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 512)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test_data['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6cae44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 628ms/step\n"
     ]
    }
   ],
   "source": [
    "ypred = model.predict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd595986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 626ms/step\n"
     ]
    }
   ],
   "source": [
    "ypred = model.predict(tokenized_test_data)\n",
    "ypred_predictions = tf.nn.softmax(ypred.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15d11da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.9741362 , 0.02586381]], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4bf98a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
